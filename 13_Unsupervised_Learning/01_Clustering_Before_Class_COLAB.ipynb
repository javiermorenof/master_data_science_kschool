{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"01_Clustering_Before_Class_COLAB.ipynb","provenance":[{"file_id":"1kg782u8hHxh95qBQFBp9KOQz7Zd6jocI","timestamp":1582239946978}],"collapsed_sections":["xi9qGJ8gRkYv"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ccLI2w38RkVH","colab_type":"text"},"source":["# Clustering\n","\n","In this notebook we will code and use different clustering techniques. The objective is to learn how to use them and understand the effects of different parameters. \n","\n","We will test:\n","\n","- kMeans\n","- hierarchical clustering\n","- DBscan"]},{"cell_type":"code","metadata":{"id":"pX5HdpAnRkVN","colab_type":"code","colab":{}},"source":["# libraries \n","\n","import numpy as np\n","import pandas as pd\n","import random \n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","np.set_printoptions(precision=4, suppress=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nxRMgPJgRkVa","colab_type":"text"},"source":["# Generate random data\n","\n","We're going to generate random data normally distributed around three centers, with noise. Each cluster will have 200 points. We concatenate all three groups in a single dataframe. "]},{"cell_type":"code","metadata":{"id":"yI4lsLnmRkVc","colab_type":"code","colab":{}},"source":["NPOINTSPERCLUSTER = 200"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cixu7LYWRkVm","colab_type":"code","colab":{}},"source":["# Set three centers\n","center_1 = np.array([0,0])\n","center_2 = np.array([3,4])\n","center_3 = np.array([6,1])\n","\n","# Generate random data around the three centers\n","data_1 = np.random.randn(NPOINTSPERCLUSTER, 2) + center_1\n","data_2 = np.random.randn(NPOINTSPERCLUSTER, 2) + center_2\n","data_3 = np.random.randn(NPOINTSPERCLUSTER, 2) + center_3\n","\n","data = np.concatenate((data_1, data_2, data_3), axis = 0)\n","print(data.shape)\n","\n","data[0:10]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YVKAtMFWRkVu","colab_type":"code","colab":{}},"source":["plt.scatter(data[:,0], data[:,1], s=7); #s=size"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vmy8rIEQl8_v","colab_type":"text"},"source":["Please, note that the data we have created does not have a class. It is just a set of points. However, we DO know that they come from different distribution and our objective is to find out them. "]},{"cell_type":"markdown","metadata":{"id":"FYFE6yC1RkV0","colab_type":"text"},"source":["# Implementation of K-Means\n","\n","In this section we will implement kmeans algorithm. \n","\n","\n","## Algorithm\n","\n","```\n","clustering (data, K):\n","    Randomly initialize K cluster centroids (mu(1),..., mu(k))\n","    # or select K random points from data\n","    Repeat until convergence:\n","        # assign cluster\n","        for d in data:\n","            assign d to closest cluster centroid\n","        # recompute cluster centroid\n","        for k = 1 to K:\n","            mu(k) = mean of data points assigned to cluster k\n","            \n","```"]},{"cell_type":"markdown","metadata":{"id":"UmBHSmUWRkV1","colab_type":"text"},"source":["### Exercise 1: complete the following code so that it follows k-means algorithm\n","\n","- Assume data is a a (Npoints, dim) numpy array. \n","- Hint: use `numpy.linalg.norm()` to compute euclidean distance\n","- Hint: `np.random.choice()` function may be useful\n","\n","*15 min*"]},{"cell_type":"code","metadata":{"id":"DNWoZ7StRkV3","colab_type":"code","colab":{}},"source":["def kmeans (data, K, plot=False):\n","    N = data.shape[0]\n","    dim = data.shape[1]\n","    \n","    # generate K random centroids\n","    centroids = 0 #TO-DO\n","    \n","    # initialize vectors\n","    new_centroids = np.zeros((K, dim))\n","    distances = np.zeros((N, K))\n","    \n","    \n","    # repeat until convergence\n","    niter = 1\n","    while True:\n","        \n","        # compute distance of each point to cluster centroid\n","        for i in range(K):\n","            distances[:,i] = 0 #TO-DO\n","         \n","        \n","        # assign points to closest centroid\n","        clusters = 0 #TO-DO\n","        \n","        # recompute clusters' centroids\n","        # Calculate mean for every cluster and update the center\n","        for i in range(K):\n","            new_centroids[i] = 0 #TO-DO\n","        \n","        # compute if there is any variation\n","        if np.array_equal(centroids, new_centroids):\n","            break\n","            \n","        centroids = new_centroids.copy()\n","\n","        if plot:\n","            # solution\n","            plt.scatter(data[:,0], data[:,1], c=clusters, cmap='plasma', linewidths=0);\n","            for k in range(K):\n","                plt.scatter(centroids[k,0], centroids[k, 1], s=100, marker='D', color='red')\n","            plt.show()\n","\n","        niter+=1\n","        \n","    return clusters, centroids    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nwej4W5URkV7","colab_type":"code","colab":{}},"source":["# let's try our algorithm\n","K = 3\n","clusters, centroids = kmeans(data, K)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cHTt5b8nRkV-","colab_type":"code","colab":{}},"source":["plt.scatter(data[:,0], data[:,1], c=clusters, cmap=\"plasma\", linewidths=0);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8c2TLG08fdPQ","colab_type":"text"},"source":["Now we are going to compare the original 'cluster' where each point comes from, with the asigned cluster. In order to do that, we just create a vector with the original class (color) and use that to plot. \n","\n","Please, recall that this is something we can do here because we're creating a synthetic dataset, but normally we won't be able to do it, since we don't know how the data has been generated. "]},{"cell_type":"code","metadata":{"id":"hEv-JQItRkWE","colab_type":"code","colab":{}},"source":["# comparison to original distribution where data came from\n","original = np.concatenate((np.repeat(0, NPOINTSPERCLUSTER), \n","                         np.repeat(1, NPOINTSPERCLUSTER), \n","                         np.repeat(2, NPOINTSPERCLUSTER)))\n","plt.scatter(data[:,0], data[:,1], c=original, cmap=\"plasma\", linewidths=0);"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2oS3psH0RkWI","colab_type":"code","colab":{}},"source":["# comparison to original distribution where data came from\n","# we create a df with the clusters and compute the 'confussion' matrix\n","\n","df = pd.DataFrame({'original' : original, 'kmeans': clusters})\n","df_g = df.groupby(['original', 'kmeans']).size().reset_index(name='n')\\\n","    .pivot(index='original', columns='kmeans', values='n').fillna(0)\n","df_g"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LTDpzY77kZU4","colab_type":"text"},"source":["Notice that class labels (kmeans) may not agree with original class number."]},{"cell_type":"code","metadata":{"id":"OKKXgIGJpesr","colab_type":"code","colab":{}},"source":["from sklearn.metrics import accuracy_score\n","\n","mapping = {i:v for (i,v) in df_g.idxmax(axis=1).items()}\n","print(mapping)\n","accuracy_score([mapping[o] for o in original], clusters)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yeAfUrMiRkWh","colab_type":"code","colab":{}},"source":["K = 3\n","clusters, centroids = kmeans(data, K, plot=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qqn3xngSRkWk","colab_type":"text"},"source":["### Exercise 2: Take some time to play with different number of (original) distributions and clusters and see the effect when number of clusters does not match true data\n","\n","15 minutes. "]},{"cell_type":"code","metadata":{"id":"dcGnW9i6RkWl","colab_type":"code","colab":{}},"source":["# Set three centers\n","center_1 = np.array([0,0])\n","center_2 = ...\n","center_3 = ...\n","...\n","\n","# Generate random data around the three centers\n","data_1 = np.random.randn(NPOINTSPERCLUSTER, 2) + center_1\n","data_2 = ...\n","... \n","\n","data = np.concatenate((data_1, data_2, ...), axis = 0)\n","\n","k = ...\n","kmeans(data, k)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BcxpF7j0RkWp","colab_type":"text"},"source":["## Computing how good the cluster partition is\n","\n","Remember SSE (Sum of Squared Error):\n","$$\n","SSE = \\sum_{i=1}^N (x_i - C_{(X_i)})^2\n","$$\n","where $C_{(X_i)}$ represents the cluster centroid of $X_i$.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"j5BcUg7sRkWp","colab_type":"text"},"source":["### Exercise 3: complete SSE function and compute clustering metrics for different number of clusters"]},{"cell_type":"code","metadata":{"id":"RLbeleqRRkWq","colab_type":"code","colab":{}},"source":["def sse(data, clusters, centroids):\n","    return 0 # TO-DO"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FmZzI-pcRkWs","colab_type":"code","colab":{}},"source":["ks = range(2, 20)\n","sse_errors = np.zeros(len(ks))\n","\n","for i, k in enumerate(ks):\n","    clusters_, centroids_ = kmeans(data, k)\n","    sse_errors[i] = sse(data, clusters_, centroids_)\n","    print(k, sse_errors[i])\n","    \n","    #if np.isnan(sse_errors[i]):\n","    #    print(clusters_)\n","    #    print(centroids_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Zw1WjK_RkWu","colab_type":"code","colab":{}},"source":["plt.plot(ks, sse_errors, 'o-');\n","plt.xlim(min(ks), max(ks));"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MTG2ekFTRkWy","colab_type":"text"},"source":["# scikit-learn K-means\n","\n","Now we are going to use scikit-learn library for our exercise\n","\n","KMeans works as other models in sklearn:\n","\n","- define the model (and parameters)\n","- fit the model on training dataset\n","- apply the fitted model on another dataset (can be the same dataset)"]},{"cell_type":"code","metadata":{"id":"X6l3CXAFRkWy","colab_type":"code","colab":{}},"source":["from sklearn.cluster import KMeans"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y8RgUjwaRkW1","colab_type":"code","colab":{}},"source":["# inspect help\n","?KMeans"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7tE6A-9TRkW3","colab_type":"code","colab":{}},"source":["# define the model\n","model = KMeans(n_clusters = 3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wlPbmj8ZRkW7","colab_type":"code","colab":{}},"source":["# fit the model on training data\n","model = model.fit(data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"esQDP_BiRkW9","colab_type":"code","colab":{}},"source":["# press tab to see available methods\n","#model. #press tab\n","print(model.cluster_centers_)\n","print(model.labels_)\n","print(model.inertia_) # sse"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4L9ewt02RkW_","colab_type":"code","colab":{}},"source":["# check inertia is our sse defined function\n","abs(model.inertia_ - sse(data, model.labels_, model.cluster_centers_)) < 0.01"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dRAWd1OARkXB","colab_type":"code","colab":{}},"source":["# apply the fitted model \n","# if applied to the same data, we get model.labels_\n","clusters_sk = model.predict(data)\n","all(model.labels_ == clusters_sk)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KY6P3MjPRkXD","colab_type":"code","colab":{}},"source":["clusters_sk"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jnydBnvwRkXF","colab_type":"code","colab":{}},"source":["def plot_clustering(data, clusters, centroids = []):\n","    \n","    K_ = len(set(clusters))\n","    \n","    plt.figure(figsize=(8,4))\n","    plt.scatter(data[:,0], data[:,1], c=clusters, cmap=\"plasma\", linewidths=0)\n","\n","    if centroids != []:\n","        for k in range(K_):\n","            plt.scatter(centroids[k,0], centroids[k, 1], s=100, marker='D', color='red')\n","        \n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g894_ywQRkXI","colab_type":"code","colab":{}},"source":["plot_clustering(data, clusters_sk)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6cBVjSZTRkXK","colab_type":"code","colab":{}},"source":["# Centroid values\n","centroids_sk = model.cluster_centers_\n","centroids_sk"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q3xSbFQ6RkXM","colab_type":"code","colab":{}},"source":["plot_clustering(data, clusters_sk, centroids_sk)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SRi2FmQsRkXP","colab_type":"code","colab":{}},"source":["# compare with our implementation\n","\n","print(centroids)\n","print(centroids_sk)\n","\n","#plot_clustering(data, clusters, centroids)\n","#plot_clustering(data, clusters_sk, centroids_sk)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qtxfxhWBRkXQ","colab_type":"text"},"source":["# Hierarchical Clustering\n","\n","In this section we will learn how to apply Hierarchical Clustering in Python. \n","\n","We will use `scipy` package:\n","\n","- `linkage` function computes the distance matrix between the points\n","- `dendrogram` function plots the dendrogram using the distances\n","- `fcluster` function performs a clustering assignment according to different parameters\n","\n","![texto alternativo](https://drive.google.com/uc?id=1MdZuOQ-SNcglxN0IXEobgrOTZR7NuaM-)"]},{"cell_type":"code","metadata":{"id":"amXQhUM2RkXR","colab_type":"code","colab":{}},"source":["from scipy.cluster.hierarchy import linkage, dendrogram, fcluster"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0dNnJXZCRkXS","colab_type":"code","colab":{}},"source":["?linkage"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"moV262tZRkXT","colab_type":"code","colab":{}},"source":["Z = linkage(data, 'ward') #'single', 'ward', ..."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W8rCf7g2RkXV","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(14, 7))\n","dendrogram(Z)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9wjy1xEdRkXY","colab_type":"text"},"source":["Now let's understand how the algorithm works. \n","\n","It follows an agglomerative approach, so *close* points are merged. \n","\n","`linkage` returns how points are merged at each iteration. The output is: [`id_node_1`, `id_node_2`, `distance`, `number_of_points_in_group`]"]},{"cell_type":"code","metadata":{"id":"Y5HZJr1GRkXZ","colab_type":"code","colab":{}},"source":["Z[0:20]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"taALLqo9RkXb","colab_type":"code","colab":{}},"source":["# indices\n","np.where(Z[:,0] > data.shape[0])[0:20]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z_aDHGECRkXd","colab_type":"code","colab":{}},"source":["Z[Z[:,0] > data.shape[0]][0:20]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V8imlLMLRkXf","colab_type":"text"},"source":["### Exercise 4: dendrogram plotting options\n","\n","Investigate dendrogram plotting options and play with them.\n","\n","- p:\n","- truncate_mode: 'lastp', 'level'\n","- color_threshold\n","- orientation\n","- count_sort: False, 'ascending'/True, 'descendent'\n","- distance_sort: False, 'ascending'/True, 'descendent'\n","- show_leaf_counts: boolean (True)\n","- show_contracted: boolean (False)\n","- above_threshold_color = 'b'\n","\n","*10 min*"]},{"cell_type":"code","metadata":{"id":"GnS9MZ62RkXg","colab_type":"code","colab":{}},"source":["# exercise\n","plt.figure(figsize=(14, 4))\n","dendrogram(Z, ...)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GQWAY_yqRkXi","colab_type":"code","colab":{}},"source":["# exercise\n","plt.figure(figsize=(14, 4))\n","dendrogram(Z, ...)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LNhkGce7RkXj","colab_type":"code","colab":{}},"source":["# exercise\n","plt.figure(figsize=(14, 4))\n","dendrogram(Z, ...)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RGOFhRN6RkXm","colab_type":"text"},"source":["## Getting the cluster partition\n","\n","In order to assign a cluster to each sample we first need to set the cut_off distance. We will visually explore what this value is and use it for partitioning. "]},{"cell_type":"code","metadata":{"id":"a53HHeaaRkXn","colab_type":"code","colab":{}},"source":["?dendrogram"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9FXLRQ6BRkXo","colab_type":"code","colab":{}},"source":["dendrogram(Z, color_threshold = 25)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mGkihBNjRkXr","colab_type":"code","colab":{}},"source":["from scipy.cluster.hierarchy import fcluster\n","\n","?fcluster"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wa-ssRI2RkXt","colab_type":"code","colab":{}},"source":["cut_distance = 25\n","clusters_hc = fcluster(Z, t = cut_distance, criterion='distance')\n","np.unique(clusters_hc, return_counts = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pqqO-y7ERkXx","colab_type":"code","colab":{}},"source":["clusters_hc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TADzcY4ZRkXz","colab_type":"text"},"source":["### Exercise 5: change cut_distance and see how the number of clusters change\n","\n","It should match the dendrogram plotting"]},{"cell_type":"code","metadata":{"id":"eN6BvIGxRkX0","colab_type":"code","colab":{}},"source":["# exercise\n","cut_distance = \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NdKR_kpHRkX5","colab_type":"text"},"source":["### Exercise 6 (home): change `method` parameter in the linkage function and observe the results"]},{"cell_type":"code","metadata":{"id":"swdJ0YlbRkX5","colab_type":"code","colab":{}},"source":["# To-Do\n","linked = linkage(data, 'to-do') #'single', 'ward', ...\n","plt.figure(figsize=(10, 5))\n","dendrogram(linked)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YnRmqswTRkX8","colab_type":"code","colab":{}},"source":["dendrogram(linked, color_threshold=cut_distance)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"odCRv-y3RkYE","colab_type":"text"},"source":["#### AgglomerativeClustering in scikit-learn\n","\n","In order to perform the clustering partition, we can also use `AgglomerativeClustering` from `sklearn` once we have selected the desired number of clusters. "]},{"cell_type":"code","metadata":{"id":"fCMI6wn7RkYF","colab_type":"code","colab":{}},"source":["from sklearn.cluster import AgglomerativeClustering\n","\n","# define the model\n","cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')  \n","\n","# fit data and predict \n","clusters = cluster.fit_predict(data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aCUO4-10RkYH","colab_type":"code","colab":{}},"source":["plot_clustering(data, clusters)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YWfymalgRkYJ","colab_type":"text"},"source":["### Comparison kmeans and hierarchical\n","\n","Now let's compare the result of both algorithms when the number of clusters is not optimal"]},{"cell_type":"code","metadata":{"id":"3fMS1hOjRkYM","colab_type":"code","colab":{}},"source":["NCLUS = 5\n","\n","km = KMeans(n_clusters=NCLUS)\n","clusters_km = km.fit_predict(data)\n","plot_clustering(data, clusters_km, km.cluster_centers_)\n","\n","hier = AgglomerativeClustering(n_clusters=NCLUS, affinity='euclidean', linkage='ward')\n","clusters_hier = hier.fit_predict(data)\n","plot_clustering(data, clusters_hier)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-EY29ZTBRkYO","colab_type":"text"},"source":["# DBSCAN\n","\n","In this section we will learn how DBscan algorithm works and what's the effect of its parameters in the clustering result. "]},{"cell_type":"code","metadata":{"id":"vXoo7ED1RkYP","colab_type":"code","colab":{}},"source":["from sklearn.cluster import DBSCAN"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h9iuj1GlRkYR","colab_type":"code","colab":{}},"source":["dbs = DBSCAN(eps=0.5, min_samples=5)\n","dbs = dbs.fit(data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8_Td5SBARkYT","colab_type":"code","colab":{}},"source":["dbs.labels_[0:50]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HGnN1EMbRkYW","colab_type":"code","colab":{}},"source":["id_clusters = np.unique(dbs.labels_)\n","print('Found {} clusters'.format(len(id_clusters)))\n","np.unique(dbs.labels_, return_counts=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AT5aHKrHRkYZ","colab_type":"code","colab":{}},"source":["plt.scatter(data[:,0], data[:,1], c=dbs.labels_, cmap=\"plasma\", linewidths=0);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fEd_RA_RRkYb","colab_type":"text"},"source":["### Exercise 7: play with the parameters `eps` and `min_samples` and see how it affects the clustering partition\n","*5 min*"]},{"cell_type":"code","metadata":{"id":"PVd4K1nNRkYc","colab_type":"code","colab":{}},"source":["# exercise\n","dbs = DBSCAN(eps=..., min_samples=...)\n","dbs = dbs.fit(data)\n","plot_clustering(data, dbs.labels_)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"FWvzFB9aRkYe","colab_type":"text"},"source":["# New data : non-convex datasets\n","\n","Now we will apply DBscan on non convex data to see the differences with k-means. We will also learn how to load already-predefined datasets from `sklearn`"]},{"cell_type":"code","metadata":{"id":"GsjPeq9xRkYe","colab_type":"code","colab":{}},"source":["from sklearn import datasets\n","\n","nsamples = 1000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_yptXpX0RkYg","colab_type":"code","colab":{}},"source":["# make_blobs\n","# this creates 'centers' circles\n","# we will change the covariance so that the clusters become ellipses\n","\n","X, y = datasets.make_blobs(random_state=170, n_samples=nsamples, centers = 5)\n","transformation = np.random.RandomState(74).normal(size=(2, 2))\n","X = np.dot(X, transformation)\n","\n","# plot\n","plt.scatter(X[:, 0], X[:, 1])\n","plt.xlabel(\"Feature 0\")\n","plt.ylabel(\"Feature 1\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V9ATiN7-RkYi","colab_type":"text"},"source":["Compare DBscan against Kmeans on that data"]},{"cell_type":"code","metadata":{"id":"Iirch9s0RkYi","colab_type":"code","colab":{}},"source":["dbscan_model = DBSCAN(eps=0.4, min_samples=5)\n","dbscan_model = dbscan_model.fit(X)\n","\n","kmeans_model = KMeans(n_clusters = 5)\n","kmeans_model = kmeans_model.fit(X)\n","\n","plot_clustering(X, dbscan_model.labels_)\n","plot_clustering(X, kmeans_model.labels_, kmeans_model.cluster_centers_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tqz8QCRjRkYk","colab_type":"code","colab":{}},"source":["# Exercise: test other datasets\n","# datasets.make_circles\n","# datasets.make_moons\n","# datasets.make_s_curve (Hint: use dimensions 0 and 2 of the generated dataset)\n","# datasets.make_swiss_roll (Hint: use dimensions 0 and 2 of the generated dataset)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k-zfFnR4RkYm","colab_type":"code","colab":{}},"source":["# make_circles\n","# ..."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_ORxk3sRRkYo","colab_type":"code","colab":{}},"source":["# make_mooons\n","# ..."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ild8b8PRkYr","colab_type":"code","colab":{}},"source":["# make_s_curve\n","# ... "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_XIPB213RkYt","colab_type":"code","colab":{}},"source":["# make_swiss_roll\n","# ..."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vjn5j-XGRkYu","colab_type":"text"},"source":["Some code and ideas are based on:\n","\n","- https://mubaris.com/posts/kmeans-clustering/\n","- https://www.kaggle.com/andyxie/k-means-clustering-implementation-in-python\n","- https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn/\n"]},{"cell_type":"markdown","metadata":{"id":"xi9qGJ8gRkYv","colab_type":"text"},"source":["### End."]}]}