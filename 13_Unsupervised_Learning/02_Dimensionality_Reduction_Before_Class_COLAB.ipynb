{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"02_Dimensionality_Reduction_Before_Class_COLAB.ipynb","provenance":[{"file_id":"1CDbaN2e5PEJY0H0KpqE4iigVJu4eCO8S","timestamp":1582287591377}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"QGz9i6ihsH59","colab_type":"text"},"source":["# Dimensionality Reduction\n","\n","- Now let's apply PCA to our dataset.\n","- We will develop our own implementation of PCA and then use scikit learn method. \n","- We will also see the effect of (un) normalized data in PCA."]},{"cell_type":"code","metadata":{"id":"UYrvVZKpsH6J","colab_type":"code","colab":{}},"source":["# libraries\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","np.set_printoptions(precision = 2, suppress=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZfmosD2EsH6e","colab_type":"text"},"source":["## Generate example data\n","\n","We are going to create random multivariate normal data specifying the correlation matrix between two dimensions."]},{"cell_type":"code","metadata":{"id":"R0KEWiulsH6k","colab_type":"code","colab":{}},"source":["original_mean = [100, 1]\n","original_covariance = [[20, 3], [3, 2]]\n","\n","X = np.random.multivariate_normal(original_mean, original_covariance, size=1000)\n","plt.scatter(X[:, 0], X[:, 1])\n","plt.xlabel(\"X\")\n","plt.ylabel(\"Y\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zAdWJRgzsH6v","colab_type":"text"},"source":["## Manual PCA\n","\n","Recall PCA steps:\n","\n","1. Compute covariance matrix\n","1. Compute eigenvectors and eigenvalues\n","1. Select top k eigenvalues (and eigenvectors)\n","1. Rotate original data with eigenvector matrix"]},{"cell_type":"code","metadata":{"id":"rfbM9O57sH6y","colab_type":"code","colab":{}},"source":["# 1. compute covariance matrix\n","# by default numpy.cov consider rows as variables, set rowvar=0\n","Sigma = np.cov(X, rowvar = 0)\n","\n","# equivalent since covariance matrix is symmetric \n","# Sigma2 = np.cov(np.transpose(X))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gDBxWELqsH67","colab_type":"code","colab":{}},"source":["# compare to original covariance matrix\n","print(X.shape)\n","print(Sigma.shape)\n","Sigma"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TBi0nTFisH7A","colab_type":"code","colab":{}},"source":["# 2. Compute eigenvalues and eigenvectors\n","eig_val, eig_vec = np.linalg.eig(Sigma)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gvg3wkbQsH7F","colab_type":"code","colab":{}},"source":["print(eig_val.shape)\n","print(eig_vec.shape)\n","eig_val"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KjmCYSnOsH7R","colab_type":"text"},"source":["**(aside) Rotate original data with eigenvector matrix to see the rotation effect**\n","\n","We may expect to see that data is now *aligned* with horizontal and vertical axis. "]},{"cell_type":"code","metadata":{"id":"5BRSUw0fsH7U","colab_type":"code","colab":{}},"source":["X_rotated = np.dot(X, eig_vec)\n","plt.scatter(X_rotated[:, 0], X_rotated[:, 1])\n","plt.xlabel(\"Feature 0\")\n","plt.ylabel(\"Feature 1\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OHxTbfgbsH7Z","colab_type":"code","colab":{}},"source":["np.cov(X_rotated, rowvar = 0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OSqCZYN0sH7e","colab_type":"text"},"source":["Let's continue with PCA!"]},{"cell_type":"code","metadata":{"id":"jsSIYfWVsH7f","colab_type":"code","colab":{}},"source":["# 2.b. according to the documentation eigenvalues does not need to be sorted, so let's do it\n","idx_sort = np.argsort(-eig_val)\n","print(idx_sort)\n","\n","eig_val_sort = eig_val[idx_sort]\n","eig_vec_sort = eig_vec[:, idx_sort]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PGibo6ZZsH7j","colab_type":"code","colab":{}},"source":["# 3- Select top k eigenvalues (and eigenvectors)\n","# Let's plot eigenvalues and percentage of total variance explained\n","\n","eig_val_sort_percentage = eig_val_sort / np.sum(eig_val_sort)\n","plt.bar(range(1,(len(eig_val)+1)), eig_val_sort_percentage); #!!!"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0qu9t1aVsH7p","colab_type":"code","colab":{}},"source":["# in this case, with only two dimensions, \n","# it is not very useful to plot the cumulative variance explained\n","\n","eig_val_sort_cum = np.cumsum(eig_val_sort) / np.sum(eig_val_sort)\n","plt.bar(range(1,(len(eig_val)+1)), eig_val_sort_cum);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XD8qFU52sH73","colab_type":"text"},"source":["In this case, we only have two dimensions, so we would select only one dimension. As data is generated just from one distribution, the output is not very meaningfull. I leave here the code, and we will test it later. "]},{"cell_type":"code","metadata":{"id":"PvHn6I32sH74","colab_type":"code","colab":{}},"source":["TOP_EIGEN = 1\n","eig_val_short = eig_val_sort[0:TOP_EIGEN]\n","eig_vec_short = eig_vec_sort[:,0:TOP_EIGEN]\n","\n","print(eig_val_short.shape)\n","print(eig_vec_short.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hzDpeYHlsH77","colab_type":"code","colab":{}},"source":["# 4. Rotate original data with reduced eigenvector matrix\n","X_pca = X.dot(eig_vec_short)\n","print(X_pca.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n_HezcmTsH7_","colab_type":"code","colab":{}},"source":["# we add some glitter to see the points\n","plt.scatter(X_pca, np.random.normal(scale=0.001, size=len(X_pca)))\n","plt.xlabel(\"Feature 0\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jfd-pwp-sH8D","colab_type":"code","colab":{}},"source":["print(np.mean(X, axis=0))\n","print(np.std(X, axis=0))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ajgxTlGTsH8Y","colab_type":"text"},"source":["# Real dataset\n","\n","Now we will use a real dataset about wine to see PCA in action. "]},{"cell_type":"code","metadata":{"id":"rHpRyZorsH8Z","colab_type":"code","colab":{}},"source":["data = pd.read_csv('wine.csv', sep='\\t')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ed7m8zrysH8c","colab_type":"code","colab":{}},"source":["data.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pj_3amC4sH8f","colab_type":"text"},"source":["We don't need the class for dimensionality reduction, so we construct a new dataframe without the column"]},{"cell_type":"code","metadata":{"id":"K7yA5IlqsH8g","colab_type":"code","colab":{}},"source":["data2 = data.loc[:,~data.columns.isin(['class'])]\n","print(data.shape)\n","print(data2.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RHzStD0zsH8k","colab_type":"text"},"source":["### Exercise: Apply PCA to this new dataset.\n","\n","- Compute the eigendecomposition and rotated data with reduced dimension. \n","- Plot two first components and add the original class as color to see if the decomposition is having any effect. \n","\n","Answer the following questions:\n","- what is the dimension of the covariance matrix?\n","- what are the eigenvalues?\n","- how much variance is explained by the first two eigenvalues?\n","\n","Note: save dimension-reduced data in variable `data_pca`"]},{"cell_type":"code","metadata":{"id":"7Lp6eEjXsH8l","colab_type":"code","colab":{}},"source":["# 1. compute covariance matrix\n","# 2. Compute eigenvalues and eigenvectors\n","# 3. Select top k eigenvalues (and eigenvectors)\n","# 4. Rotate original data with eigenvector matrix\n","# 5. Plot two first dimensions of rotated data with the color being the original label"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RE6eLdoAsH8p","colab_type":"code","colab":{}},"source":["# to-do\n","# ..."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bLUHllklsH8t","colab_type":"code","colab":{}},"source":["# ..."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fKUtepXksH8x","colab_type":"code","colab":{}},"source":["# ..."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gsZpQiJ-sH82","colab_type":"text"},"source":["## scikit-learn PCA"]},{"cell_type":"code","metadata":{"id":"9fnelhEHsH83","colab_type":"code","colab":{}},"source":["from sklearn.decomposition import PCA"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aIINGbVpsH85","colab_type":"code","colab":{}},"source":["pca_model = PCA()\n","data_pca_sklearn = pca_model.fit_transform(data_norm)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I-bNdenSsH89","colab_type":"code","colab":{}},"source":["print('Default number of components: {}'.format(pca_model.n_components_))\n","print(pca_model.explained_variance_)\n","print(pca_model.explained_variance_ratio_)\n","print(np.cumsum(pca_model.explained_variance_ratio_))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X0Vaw4VVsH9C","colab_type":"code","colab":{}},"source":["pca_model = PCA(n_components = 5)\n","data_pca_sklearn = pca_model.fit_transform(data_norm)\n","\n","print(pca_model.explained_variance_)\n","print(pca_model.explained_variance_ratio_)\n","print(np.cumsum(pca_model.explained_variance_ratio_))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wMrpa6A7sH9F","colab_type":"code","colab":{}},"source":["print(data_pca_sklearn[0:2])\n","print(data_pca[0:2])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rTYGPCiOsH9I","colab_type":"code","colab":{}},"source":["plt.scatter(data_pca_sklearn[:,0], data_pca_sklearn[:,1], c=data['class'], cmap=\"plasma\", linewidths=0);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gbU40Fi3sH9L","colab_type":"text"},"source":["#### [Home] Exercise\n","Apply your favourite(s) ML algorithm to estimate the 'class' with the other parameters. Test if you appreciate any difference when applying PCA to the features and later apply your model. \n","\n","- performance?\n","- computational time?\n","- memory requirements?"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"XpGIvplOsH9M","colab_type":"text"},"source":["# END."]}]}